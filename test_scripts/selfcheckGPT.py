# Include necessary packages (torch, spacy, ...)
from selfcheckgpt.modeling_selfcheck import SelfCheckMQAG, SelfCheckBERTScore, SelfCheckNgram
from selfcheckgpt.modeling_selfcheck import SelfCheckNLI
import torch
import spacy
import os
from dotenv import load_dotenv, find_dotenv
load_dotenv(find_dotenv())
MODEL_ROOT = os.environ.get("model_path")

nlp = spacy.load(f"{MODEL_ROOT}/en_core_web_sm-3.6.0/en_core_web_sm/en_core_web_sm-3.6.0")
# --------------------------------------------------------------------------------------------------------------- #
# Download NLI model from https://huggingface.co/potsawee/deberta-v3-large-mnli and specify the model path by SelfCheckNLI(nli_model=MODEL_PATH), or it will be auto downloaded.
# Download spacy model by running `python -m spacy download en_core_web_sm`
# --------------------------------------------------------------------------------------------------------------- #
from typing import List, Literal

METHOD: Literal['mqag', 'bertscore', 'ngram', 'nli'] = 'nli'
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# LLM's text (e.g. GPT-3 response) to be evaluated at the sentence level  & Split it into sentences
passage = "Michael Alan Weiner (born March 31, 1942) is an American radio host. He is the host of The Savage Nation."
sentences = [sent.text.strip() for sent in nlp(passage).sents] # spacy sentence tokenization
print(sentences)
# ['Michael Alan Weiner (born March 31, 1942) is an American radio host.', 'He is the host of The Savage Nation.']

# Other samples generated by the same LLM to perform self-check for consistency
sample1 = "Michael Alan Weiner (born March 31, 1942) is an American radio host. He is the host of The Savage Country."
sample2 = "Michael Alan Weiner (born January 13, 1960) is a Canadian radio host. He works at The New York Times."
sample3 = "Michael Alan Weiner (born March 31, 1942) is an American radio host. He obtained his PhD from MIT."


if METHOD == 'mqag':
    selfcheck_mqag = SelfCheckMQAG(device=device)
    # --------------------------------------------------------------------------------------------------------------- #
    # SelfCheck-MQAG: Score for each sentence where value is in [0.0, 1.0] and high value means non-factual
    # Additional params for each scoring_method:
    # -> counting: AT (answerability threshold, i.e. questions with answerability_score < AT are rejected)
    # -> bayes: AT, beta1, beta2
    # -> bayes_with_alpha: beta1, beta2
    sent_scores_mqag = selfcheck_mqag.predict(
        sentences = sentences,               # list of sentences
        passage = passage,                   # passage (before sentence-split)
        sampled_passages = [sample1, sample2, sample3], # list of sampled passages
        num_questions_per_sent = 5,          # number of questions to be drawn  
        scoring_method = 'bayes_with_alpha', # options = 'counting', 'bayes', 'bayes_with_alpha'
        beta1 = 0.8, beta2 = 0.8,            # additional params depending on scoring_method
    )
    print(sent_scores_mqag)
    # [0.30990949 0.42376232]
elif METHOD == 'bertscore':
    selfcheck_bertscore = SelfCheckBERTScore(rescale_with_baseline=True)
    # --------------------------------------------------------------------------------------------------------------- #
    # SelfCheck-BERTScore: Score for each sentence where value is in [0.0, 1.0] and high value means non-factual
    sent_scores_bertscore = selfcheck_bertscore.predict(
        sentences = sentences,                          # list of sentences
        sampled_passages = [sample1, sample2, sample3], # list of sampled passages
    )
    print(sent_scores_bertscore)
    # [0.0695562  0.45590915]

elif METHOD == 'ngram':
    selfcheck_ngram = SelfCheckNgram(n=1) # n=1 means Unigram, n=2 means Bigram, etc.
    # --------------------------------------------------------------------------------------------------------------- #
    # SelfCheck-Ngram: Score at sentence- and document-level where value is in [0.0, +inf) and high value means non-factual
    # as opposed to SelfCheck-MQAG and SelfCheck-BERTScore, SelfCheck-Ngram's score is not bounded
    sent_scores_ngram = selfcheck_ngram.predict(
        sentences = sentences,   
        passage = passage,
        sampled_passages = [sample1, sample2, sample3],
    )
    print(sent_scores_ngram)
    # {'sent_level': { # sentence-level score similar to MQAG and BERTScore variant
    #     'avg_neg_logprob': [3.184312, 3.279774],
    #     'max_neg_logprob': [3.476098, 4.574710]
    #     },
    #  'doc_level': {  # document-level score such that avg_neg_logprob is computed over all tokens
    #     'avg_neg_logprob': 3.218678904916201,
    #     'avg_max_neg_logprob': 4.025404834169327
    #     }
    # }
elif METHOD == 'nli':
    selfcheck_nli = SelfCheckNLI(nli_model="{MODEL_ROOT}/deberta-v3-large-mnli",device=device) # set device to 'cuda' if GPU is available

    sent_scores_nli = selfcheck_nli.predict(
        sentences = sentences,                          # list of sentences
        sampled_passages = [sample1, sample2, sample3], # list of sampled passages
    )
    print(sent_scores_nli)
    # [0.334014 0.975106 ] -- based on the example above 
    # scores of Contradiction of the two sents wrt to the samples.